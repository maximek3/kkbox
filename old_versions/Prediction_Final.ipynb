{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# machine learning library\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "#import XGBOOST Libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# plots\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Configure Panda\n",
    "pd.options.display.width = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and pre-processing data\n",
    "### 2.1 Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "# This is the input for our model\n",
    "\n",
    "transactions_feb_untweaked = pd.read_csv('data/transactions_feb.csv')\n",
    "transactions_mar_untweaked = pd.read_csv('data/transactions_mar_train_v2.csv')\n",
    "transactions_apr_untweaked = pd.read_csv('data/transactions_apr.csv')\n",
    "members_feb_untweaked = pd.read_csv('data/members_train1.csv')\n",
    "members_mar_untweaked = pd.read_csv('data/members_train2.csv')\n",
    "members_apr_untweaked = pd.read_csv('data/members_total.csv')\n",
    "user_logs_feb_untweaked = pd.read_csv('data/user_logs_feb.csv')\n",
    "user_logs_mar_untweaked = pd.read_csv('data/user_logs_mar.csv')\n",
    "user_logs_apr_untweaked = pd.read_csv('data/user_logs_apr.csv')\n",
    "\n",
    "print('data loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del user_logs_feb['pVpp_entries']\n",
    "del user_logs_mar['pVpp_entries']\n",
    "del user_logs_apr['pVpp_entries']\n",
    "del user_logs_feb['pVpp_secs']\n",
    "del user_logs_mar['pVpp_secs']\n",
    "del user_logs_apr['pVpp_secs']\n",
    "\n",
    "del user_logs_feb['entries_m2']\n",
    "del user_logs_mar['entries_m2']\n",
    "del user_logs_apr['entries_m2']\n",
    "del user_logs_feb['secs_m2']\n",
    "del user_logs_mar['secs_m2']\n",
    "del user_logs_apr['secs_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 21)\n",
      "(970960, 21)\n",
      "(907471, 21)\n",
      "(992931, 35)\n",
      "(970960, 35)\n",
      "(907471, 35)\n",
      "(992931, 28)\n",
      "(970960, 28)\n",
      "(907471, 28)\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_feb_untweaked.shape)\n",
    "print(user_logs_mar_untweaked.shape)\n",
    "print(user_logs_apr_untweaked.shape)\n",
    "print(members_feb_untweaked.shape)\n",
    "print(members_mar_untweaked.shape)\n",
    "print(members_apr_untweaked.shape)\n",
    "print(transactions_feb_untweaked.shape)\n",
    "print(transactions_mar_untweaked.shape)\n",
    "print(transactions_apr_untweaked.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs_feb_tweak = user_logs_feb_untweaked.copy()\n",
    "user_logs_mar_tweak = user_logs_mar_untweaked.copy()\n",
    "user_logs_apr_tweak = user_logs_apr_untweaked.copy()\n",
    "\n",
    "members_feb_tweak = members_feb_untweaked.copy()\n",
    "members_mar_tweak = members_mar_untweaked.copy()\n",
    "members_apr_tweak = members_apr_untweaked.copy()\n",
    "\n",
    "transactions_feb_tweak = transactions_feb_untweaked.copy()\n",
    "transactions_mar_tweak = transactions_mar_untweaked.copy()\n",
    "transactions_apr_tweak = transactions_apr_untweaked.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, ignore this part. We could use it to test the algorithm out by dropping a few features and thereby imrpove the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empirical feature selection improves the result\n",
    "#user_logs_feb_tweak = user_logs_feb_tweak.drop(['entries_m3','entries_m2','entries_m1','secs_m3','secs_m2','secs_m1'],axis=1)\n",
    "#user_logs_mar_tweak = user_logs_mar_tweak.drop(['entries_m3','entries_m2','entries_m1','secs_m3','secs_m2','secs_m1'],axis=1)\n",
    "#user_logs_apr_tweak = user_logs_apr_tweak.drop(['entries_m3','entries_m2','entries_m1','secs_m3','secs_m2','secs_m1'],axis=1)\n",
    "\n",
    "#members_feb_tweak = members_feb_tweak[['city_1','city_3','city_4','city_22','bd_norm','reg_year_2017','reg_year_2016','reg_year_2015','msno','is_churn']]\n",
    "#members_mar_tweak = members_mar_tweak[['city_1','city_3','city_4','city_22','bd_norm','reg_year_2017','reg_year_2016','reg_year_2015','msno','is_churn']]\n",
    "#members_apr_tweak = members_apr_tweak[['city_1','city_3','city_4','city_22','bd_norm','reg_year_2017','reg_year_2016','reg_year_2015','msno','is_churn']]\n",
    "\n",
    "#transactions_feb_tweak=transactions_feb_tweak[['payment_plan_days_30','transactions','is_cancel','is_auto_renew','msno','is_churn']]\n",
    "#transactions_mar_tweak=transactions_mar_tweak[['payment_plan_days_30','transactions','is_cancel','is_auto_renew','msno','is_churn']]\n",
    "#transactions_apr_tweak=transactions_apr_tweak[['payment_plan_days_30','transactions','is_cancel','is_auto_renew','msno','is_churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 21)\n",
      "(970960, 21)\n",
      "(907471, 21)\n",
      "(992931, 35)\n",
      "(970960, 35)\n",
      "(907471, 35)\n",
      "(992931, 28)\n",
      "(970960, 28)\n",
      "(907471, 28)\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_feb_tweak.shape)\n",
    "print(user_logs_mar_tweak.shape)\n",
    "print(user_logs_apr_tweak.shape)\n",
    "print(members_feb_tweak.shape)\n",
    "print(members_mar_tweak.shape)\n",
    "print(members_apr_tweak.shape)\n",
    "print(transactions_feb_tweak.shape)\n",
    "print(transactions_mar_tweak.shape)\n",
    "print(transactions_apr_tweak.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the different files containing the features are merged with the train and test data. Here, we need to have a look at the specifics of the problem we actually need to solve. We are given a training data set for february and march with the corresponding response variable \"is_churn\". We need to use this data and make a prediction for the month of april. To get an estimation of our performance, we are going to train the data on february and test it on march. For the month of april, we are going to train on the entire data, i.e. February and March. The prediction will then be made for april."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_data = pd.merge(members_feb_tweak,user_logs_feb_tweak.drop('is_churn',axis=1),on='msno')\n",
    "feb_data = pd.merge(feb_data,transactions_feb_tweak.drop('is_churn',axis=1),on='msno')\n",
    "mar_data = pd.merge(members_mar_tweak,user_logs_mar_tweak.drop('is_churn',axis=1),on='msno')\n",
    "mar_data = pd.merge(mar_data,transactions_mar_tweak.drop('is_churn',axis=1),on='msno')\n",
    "apr_data = pd.merge(members_apr_tweak,user_logs_apr_tweak.drop('is_churn',axis=1),on='msno')\n",
    "apr_data = pd.merge(apr_data,transactions_apr_tweak.drop('is_churn',axis=1),on='msno')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 80)\n",
      "(970960, 80)\n",
      "(907471, 80)\n"
     ]
    }
   ],
   "source": [
    "print(feb_data.shape)\n",
    "print(mar_data.shape)\n",
    "print(apr_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure each dataset has the same order of it's columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mar_data = mar_data[feb_data.columns]\n",
    "apr_data = apr_data[mar_data.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def lin_reg(train,test):\n",
    "    if ('msno' in train.columns):\n",
    "        train = train.drop('msno',axis=1)\n",
    "    if ('msno' in test.columns):\n",
    "        test = test.drop('msno',axis=1)\n",
    "    X_train = train.drop('is_churn',axis=1)\n",
    "    y_train = train['is_churn']\n",
    "    X_test = test.drop('is_churn',axis=1)\n",
    "    y_test = test['is_churn']\n",
    "    \n",
    "    model_lin_reg = LinearRegression()\n",
    "    model_lin_reg.fit(X_train, y_train)\n",
    "    y_pred_l = model_lin_reg.predict(X_test)\n",
    "    y_pred_l[y_pred_l<0] = 0\n",
    "    print(\"Logloss for Linear Regression is: %.6f\"%log_loss(y_test,y_pred_l))\n",
    "    #y_pred_l[y_pred_l>=0.95] = 1\n",
    "    #y_pred_l[y_pred_l<0.05] = 0\n",
    "    #print(\"(1 or 0) Logloss for Linear Regression is: %.6f\"%log_loss(y_test,y_pred_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for Linear Regression is: 0.521153\n",
      "### over-fitting check: ###\n",
      "Logloss for Linear Regression is: 0.328021\n"
     ]
    }
   ],
   "source": [
    "lin_reg(feb_data,mar_data)\n",
    "print('### over-fitting check: ###')\n",
    "lin_reg(feb_data,feb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Forest\n",
    "\n",
    "def ran_for(train,test,importance):\n",
    "    start = time.time()\n",
    "    if ('msno' in train.columns):\n",
    "        train = train.drop('msno',axis=1)\n",
    "    if ('msno' in test.columns):\n",
    "        test = test.drop('msno',axis=1)\n",
    "    X_train = train.drop('is_churn',axis=1)\n",
    "    y_train = train['is_churn']\n",
    "    X_test = test.drop('is_churn',axis=1)\n",
    "    y_test = test['is_churn']\n",
    "    \n",
    "    model_Forest = RandomForestRegressor()\n",
    "    model_Forest.fit(X_train, y_train)\n",
    "    y_pred_f = model_Forest.predict(X_test)\n",
    "    print(\"Logloss for Random Forrest is: %.6f\"%log_loss(y_test,y_pred_f))\n",
    "    \n",
    "    if (importance == 1):\n",
    "        importances = model_Forest.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        columns = np.array(list(X_train))\n",
    "        return importances\n",
    "        \n",
    "        # Print the feature ranking\n",
    "        print(\"\\nFeature ranking:\")\n",
    "        \n",
    "        for f in range(x_train.shape[1]):\n",
    "            print(\"%d. %s (%f)\" % (f + 1, columns[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "    print('RF Time = %.0f'%(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for Random Forrest is: 0.939666\n",
      "### over-fitting check: ###\n",
      "Logloss for Random Forrest is: 0.048585\n",
      "RF Time = 447\n"
     ]
    }
   ],
   "source": [
    "ran_for(feb_data,mar_data,1)\n",
    "print('### over-fitting check: ###')\n",
    "ran_for(feb_data,feb_data,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XGD Boost\n",
    "def xgb_boost(train,test,importance,final):\n",
    "    start = time.time()\n",
    "    if ('msno' in train.columns):\n",
    "        train = train.drop('msno',axis=1)\n",
    "    if ('msno' in test.columns):\n",
    "        test = test.drop('msno',axis=1)\n",
    "    X_train = train.drop('is_churn',axis=1)\n",
    "    y_train = train['is_churn']\n",
    "    X_test = test.drop('is_churn',axis=1)\n",
    "    y_test = test['is_churn']\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "    param = {\n",
    "        'max_depth': 3,  # the maximum depth of each tree. Try with max_depth: 2 to 10.\n",
    "        'eta': 0.3,  # the training step for each iteration. Try with ETA: 0.1, 0.2, 0.3...\n",
    "        'silent': 1,  # logging mode - quiet\n",
    "        'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "        'num_class': 3}  # the number of classes that exist in this datset\n",
    "    num_round = 20  # the number of training iterations. Try with num_round around few hundred!\n",
    "    #----------------\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    y_pred_xgb = bst.predict(dtest)\n",
    "    best_preds = np.asarray([np.argmax(line) for line in y_pred_xgb])\n",
    "    y_pred_xgb = y_pred_xgb[:,1] #Column 2 out of 3\n",
    "    if (final == 1):\n",
    "        return y_pred_xgb\n",
    "    print(\"(probs) Logloss for XGD Boost is: %.6f\"%log_loss(y_test,y_pred_xgb))\n",
    "    \n",
    "    if (importance == 1):\n",
    "        xgb.plot_importance(bst,max_num_features=10)\n",
    "        plt.show()\n",
    "    \n",
    "    #y_pred_xgb[y_pred_xgb>=0.5] = 1\n",
    "    #y_pred_xgb[y_pred_xgb<0.5] = 0\n",
    "    #print(\"(1 or 0) Logloss for XGD Boost is: %.6f\"%log_loss(y_test,y_pred_xgb))\n",
    "\n",
    "    print('XGB Time = %.0f'%(time.time() - start))\n",
    "    \n",
    "    return y_pred_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_boost(feb_data,mar_data,1,0)\n",
    "print('### over-fitting check: ###')\n",
    "xgb_boost(feb_data,feb_data,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def neural(train,test,cv):\n",
    "    start = time.time()\n",
    "    if ('msno' in train.columns):\n",
    "        train = train.drop('msno',axis=1)\n",
    "    if ('msno' in test.columns):\n",
    "        test = test.drop('msno',axis=1)\n",
    "    X_train = train.drop('is_churn',axis=1)\n",
    "    y_train = train['is_churn']\n",
    "    X_test = test.drop('is_churn',axis=1)\n",
    "    y_test = test['is_churn']\n",
    "\n",
    "    model_n = MLPClassifier()\n",
    "    model_n.fit(X_train, y_train)\n",
    "    y_pred_n = model_n.predict(X_test)\n",
    "    \n",
    "    print(\"Logloss for Neural Network is: %.6f\"%log_loss(y_test,y_pred_n)) \n",
    "    \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores = cross_val_score(model_n, data_input, data_output, cv=cv, scoring='log_loss')\n",
    "        #scores = - scores\n",
    "        print(scores)\n",
    "        print(\"CV log loss: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    print('Neural Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for Neural Network is: 8.863182\n",
      "Neural Time = 23\n",
      "### over-fitting check: ###\n",
      "Logloss for Neural Network is: 2.196548\n",
      "Neural Time = 41\n"
     ]
    }
   ],
   "source": [
    "neural(feb_data,mar_data,0)\n",
    "print('### over-fitting check: ###')\n",
    "neural(feb_data,feb_data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "def ada_b(train,test):\n",
    "    start = time.time()\n",
    "    if ('msno' in train.columns):\n",
    "        train = train.drop('msno',axis=1)\n",
    "    if ('msno' in test.columns):\n",
    "        test = test.drop('msno',axis=1)\n",
    "    X_train = train.drop('is_churn',axis=1)\n",
    "    y_train = train['is_churn']\n",
    "    X_test = test.drop('is_churn',axis=1)\n",
    "    y_test = test['is_churn']\n",
    "    \n",
    "    model_abr = AdaBoostRegressor()\n",
    "    model_abr.fit(X_train, y_train)\n",
    "    print('Model fitted!')\n",
    "    y_pred_abr = model_abr.predict(X_test)\n",
    "    print(\"Logloss for AdaBoost is: %.6f\"%log_loss(y_test,y_pred_abr))\n",
    "    print('Time =',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted!\n",
      "Logloss for AdaBoost is: 0.241840\n",
      "Time = 140.39595460891724\n",
      "### over-fitting check: ###\n",
      "Model fitted!\n",
      "Logloss for AdaBoost is: 0.183722\n",
      "Time = 119.57487607002258\n"
     ]
    }
   ],
   "source": [
    "ada_b(feb_data,mar_data)\n",
    "print('### over-fitting check: ###')\n",
    "ada_b(feb_data,feb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As user_logs data contains daterelated features that are related to the date where is_churn is taken from, and we have is_churn at two different dates, we can append mar_data and feb_data and thus will have a trainset of double the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = mar_data.append(feb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create the final submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_churn                                          msno\n",
      "0  0.026196  4n+fXlyJvfQnTeKXTWT507Ll4JVYGrOC8LHCfwBmPE4=\n",
      "1  0.016431  aNmbC1GvFUxQyQUidCVmfbQ0YeCuwkPzEdQ0RwWyeZM=\n",
      "2  0.072834  rFC9eSG/tMuzpre6cwcMLZHEYM89xY02qcz7HL4//jc=\n",
      "3  0.023867  WZ59dLyrQcE7ft06MZ5dj40BnlYQY7PHgg/54+HaCSE=\n",
      "4  0.068619  aky/Iv8hMp1/V/yQHLtaVuEmmAxkB5GuasQZePJ7NU4=\n",
      "                                           msno  is_churn\n",
      "0  4n+fXlyJvfQnTeKXTWT507Ll4JVYGrOC8LHCfwBmPE4=  0.026196\n",
      "1  aNmbC1GvFUxQyQUidCVmfbQ0YeCuwkPzEdQ0RwWyeZM=  0.016431\n",
      "2  rFC9eSG/tMuzpre6cwcMLZHEYM89xY02qcz7HL4//jc=  0.072834\n",
      "3  WZ59dLyrQcE7ft06MZ5dj40BnlYQY7PHgg/54+HaCSE=  0.023867\n",
      "4  aky/Iv8hMp1/V/yQHLtaVuEmmAxkB5GuasQZePJ7NU4=  0.068619\n",
      "msno        907471\n",
      "is_churn    907471\n",
      "dtype: int64\n",
      "Done! :-)\n"
     ]
    }
   ],
   "source": [
    "#Prepare submission file\n",
    "y_pred_xgb = xgb_boost(total_data,apr_data,0,1)\n",
    "my_submission = pd.DataFrame({'msno': apr_data.msno, 'is_churn': y_pred_xgb})\n",
    "print(my_submission.head())\n",
    "cols = my_submission.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "my_submission = my_submission[cols]\n",
    "print(my_submission.head())\n",
    "print(my_submission.count())\n",
    "\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "print('Done! :-)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
