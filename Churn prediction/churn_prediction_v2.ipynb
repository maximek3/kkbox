{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Solution (for transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning library\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#import XGBOOST Libraries\n",
    "#import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#Configure Panda\n",
    "pd.options.display.width = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and pre-processing data\n",
    "### !!! ATTENTION: In order to load the following files you first need to completely run Feature_Engineering_Members.ipynb and Feature_Engineering_Transactions.ipynb. !!!\n",
    "### 2.1 Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load data in\n",
    "train = pd.read_csv('data/train_v2.csv')\n",
    "test = pd.read_csv('data/sample_submission_v2.csv')\n",
    "transactions = pd.read_csv('data/final_transactions.csv')\n",
    "final_members=pd.read_csv('data/final_members.csv')\n",
    "user_logs = pd.read_csv('data/final_user_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6769473, 24)\n",
      "(1431009, 125)\n",
      "(495677, 7)\n",
      "(970960, 2)\n"
     ]
    }
   ],
   "source": [
    "print(final_members.shape)\n",
    "print(transactions.shape)\n",
    "print(user_logs.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data as 970960 different msnos\n"
     ]
    }
   ],
   "source": [
    "print('The train data as',train['msno'].count(),'different msnos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge the different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets witgh input&outputs\n",
    "train_data = pd.merge(train, final_members, on='msno', how='left')\n",
    "train_data = pd.merge(train_data, transactions, how='left',on='msno',left_index=True, right_index=True)\n",
    "#train_data = pd.merge(train_data, user_logs, how='left',on='msno',left_index=True, right_index=True)\n",
    "\n",
    "train_data_logs = pd.merge(train, final_members, on='msno', how='left')\n",
    "train_data_logs = pd.merge(train_data_logs, transactions, how='left',on='msno',left_index=True, right_index=True)\n",
    "train_data_logs = pd.merge(train_data_logs, user_logs, on='msno',left_index=True, right_index=True)\n",
    "\n",
    "# Creating datasets with only inputs\n",
    "# Note that the submission_v2.csv file does NOT contain ouputs (they are all 0)\n",
    "test_data = pd.merge(test,final_members, on='msno', how='left')\n",
    "test_data = pd.merge(test_data,transactions, how='left',on='msno',left_index=True, right_index=True)\n",
    "\n",
    "test_data_logs = pd.merge(test,final_members, on='msno', how='left')\n",
    "test_data_logs = pd.merge(test_data,transactions, how='left',on='msno',left_index=True, right_index=True)\n",
    "test_data_logs = pd.merge(test_data, user_logs ,how='left',on='msno',left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(907471, 149)\n",
      "(970960, 149)\n",
      "(495677, 155)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(train_data.shape)\n",
    "print(train_data_logs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has 970960 different msnos\n",
      "The test data has 907471 different msnos\n",
      "The members data has 6769473 different msnos\n",
      "The transactions data has 1431009 different msnos\n",
      "The user logs data has 495677 different msnos\n",
      "Compared to that, the altered test data, excluding logs, has 907471 different msnos\n"
     ]
    }
   ],
   "source": [
    "print('The train data has',train['msno'].count(),'different msnos')\n",
    "print('The test data has',test['msno'].count(),'different msnos')\n",
    "print('The members data has',final_members['msno'].count(),'different msnos')\n",
    "print('The transactions data has',transactions['msno'].count(),'different msnos')\n",
    "print('The user logs data has',user_logs['msno'].count(),'different msnos')\n",
    "print('Compared to that, the altered test data, excluding logs, has',test_data['msno'].count(),'different msnos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno                           0\n",
      "is_churn                       0\n",
      "city_1                    109993\n",
      "city_3                    109993\n",
      "city_4                    109993\n",
      "city_5                    109993\n",
      "city_6                    109993\n",
      "city_7                    109993\n",
      "city_8                    109993\n",
      "city_9                    109993\n",
      "city_10                   109993\n",
      "city_11                   109993\n",
      "city_12                   109993\n",
      "city_13                   109993\n",
      "city_14                   109993\n",
      "city_15                   109993\n",
      "city_16                   109993\n",
      "city_17                   109993\n",
      "city_18                   109993\n",
      "city_19                   109993\n",
      "city_20                   109993\n",
      "city_21                   109993\n",
      "city_22                   109993\n",
      "reg_name                  109993\n",
      "reg_month                 109993\n",
      "actual_amount_paid             0\n",
      "is_auto_renew                  0\n",
      "transaction_date               0\n",
      "membership_expire_date         0\n",
      "is_cancel                      0\n",
      "                           ...  \n",
      "plan_list_price_150            0\n",
      "plan_list_price_180            0\n",
      "plan_list_price_210            0\n",
      "plan_list_price_265            0\n",
      "plan_list_price_298            0\n",
      "plan_list_price_300            0\n",
      "plan_list_price_350            0\n",
      "plan_list_price_400            0\n",
      "plan_list_price_447            0\n",
      "plan_list_price_450            0\n",
      "plan_list_price_477            0\n",
      "plan_list_price_480            0\n",
      "plan_list_price_500            0\n",
      "plan_list_price_536            0\n",
      "plan_list_price_596            0\n",
      "plan_list_price_600            0\n",
      "plan_list_price_699            0\n",
      "plan_list_price_799            0\n",
      "plan_list_price_894            0\n",
      "plan_list_price_930            0\n",
      "plan_list_price_1000           0\n",
      "plan_list_price_1150           0\n",
      "plan_list_price_1200           0\n",
      "plan_list_price_1260           0\n",
      "plan_list_price_1299           0\n",
      "plan_list_price_1300           0\n",
      "plan_list_price_1399           0\n",
      "plan_list_price_1599           0\n",
      "plan_list_price_1788           0\n",
      "plan_list_price_2000           0\n",
      "Length: 149, dtype: int64\n",
      "msno                          0\n",
      "is_churn                      0\n",
      "city_1                    55615\n",
      "city_3                    55615\n",
      "city_4                    55615\n",
      "city_5                    55615\n",
      "city_6                    55615\n",
      "city_7                    55615\n",
      "city_8                    55615\n",
      "city_9                    55615\n",
      "city_10                   55615\n",
      "city_11                   55615\n",
      "city_12                   55615\n",
      "city_13                   55615\n",
      "city_14                   55615\n",
      "city_15                   55615\n",
      "city_16                   55615\n",
      "city_17                   55615\n",
      "city_18                   55615\n",
      "city_19                   55615\n",
      "city_20                   55615\n",
      "city_21                   55615\n",
      "city_22                   55615\n",
      "reg_name                  55615\n",
      "reg_month                 55615\n",
      "actual_amount_paid            0\n",
      "is_auto_renew                 0\n",
      "transaction_date              0\n",
      "membership_expire_date        0\n",
      "is_cancel                     0\n",
      "                          ...  \n",
      "plan_list_price_350           0\n",
      "plan_list_price_400           0\n",
      "plan_list_price_447           0\n",
      "plan_list_price_450           0\n",
      "plan_list_price_477           0\n",
      "plan_list_price_480           0\n",
      "plan_list_price_500           0\n",
      "plan_list_price_536           0\n",
      "plan_list_price_596           0\n",
      "plan_list_price_600           0\n",
      "plan_list_price_699           0\n",
      "plan_list_price_799           0\n",
      "plan_list_price_894           0\n",
      "plan_list_price_930           0\n",
      "plan_list_price_1000          0\n",
      "plan_list_price_1150          0\n",
      "plan_list_price_1200          0\n",
      "plan_list_price_1260          0\n",
      "plan_list_price_1299          0\n",
      "plan_list_price_1300          0\n",
      "plan_list_price_1399          0\n",
      "plan_list_price_1599          0\n",
      "plan_list_price_1788          0\n",
      "plan_list_price_2000          0\n",
      "num_100                       0\n",
      "entries                       0\n",
      "num25_ratio                   0\n",
      "num50_ratio                   0\n",
      "num75_ratio                   0\n",
      "num985_ratio                  0\n",
      "Length: 155, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for null values\n",
    "print(train_data.isnull().sum())\n",
    "print(train_data_logs.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno                      0\n",
      "is_churn                  0\n",
      "city_1                    0\n",
      "city_3                    0\n",
      "city_4                    0\n",
      "city_5                    0\n",
      "city_6                    0\n",
      "city_7                    0\n",
      "city_8                    0\n",
      "city_9                    0\n",
      "city_10                   0\n",
      "city_11                   0\n",
      "city_12                   0\n",
      "city_13                   0\n",
      "city_14                   0\n",
      "city_15                   0\n",
      "city_16                   0\n",
      "city_17                   0\n",
      "city_18                   0\n",
      "city_19                   0\n",
      "city_20                   0\n",
      "city_21                   0\n",
      "city_22                   0\n",
      "reg_name                  0\n",
      "reg_month                 0\n",
      "actual_amount_paid        0\n",
      "is_auto_renew             0\n",
      "transaction_date          0\n",
      "membership_expire_date    0\n",
      "is_cancel                 0\n",
      "                         ..\n",
      "plan_list_price_150       0\n",
      "plan_list_price_180       0\n",
      "plan_list_price_210       0\n",
      "plan_list_price_265       0\n",
      "plan_list_price_298       0\n",
      "plan_list_price_300       0\n",
      "plan_list_price_350       0\n",
      "plan_list_price_400       0\n",
      "plan_list_price_447       0\n",
      "plan_list_price_450       0\n",
      "plan_list_price_477       0\n",
      "plan_list_price_480       0\n",
      "plan_list_price_500       0\n",
      "plan_list_price_536       0\n",
      "plan_list_price_596       0\n",
      "plan_list_price_600       0\n",
      "plan_list_price_699       0\n",
      "plan_list_price_799       0\n",
      "plan_list_price_894       0\n",
      "plan_list_price_930       0\n",
      "plan_list_price_1000      0\n",
      "plan_list_price_1150      0\n",
      "plan_list_price_1200      0\n",
      "plan_list_price_1260      0\n",
      "plan_list_price_1299      0\n",
      "plan_list_price_1300      0\n",
      "plan_list_price_1399      0\n",
      "plan_list_price_1599      0\n",
      "plan_list_price_1788      0\n",
      "plan_list_price_2000      0\n",
      "Length: 149, dtype: int64\n",
      "msno                      0\n",
      "is_churn                  0\n",
      "city_1                    0\n",
      "city_3                    0\n",
      "city_4                    0\n",
      "city_5                    0\n",
      "city_6                    0\n",
      "city_7                    0\n",
      "city_8                    0\n",
      "city_9                    0\n",
      "city_10                   0\n",
      "city_11                   0\n",
      "city_12                   0\n",
      "city_13                   0\n",
      "city_14                   0\n",
      "city_15                   0\n",
      "city_16                   0\n",
      "city_17                   0\n",
      "city_18                   0\n",
      "city_19                   0\n",
      "city_20                   0\n",
      "city_21                   0\n",
      "city_22                   0\n",
      "reg_name                  0\n",
      "reg_month                 0\n",
      "actual_amount_paid        0\n",
      "is_auto_renew             0\n",
      "transaction_date          0\n",
      "membership_expire_date    0\n",
      "is_cancel                 0\n",
      "                         ..\n",
      "plan_list_price_150       0\n",
      "plan_list_price_180       0\n",
      "plan_list_price_210       0\n",
      "plan_list_price_265       0\n",
      "plan_list_price_298       0\n",
      "plan_list_price_300       0\n",
      "plan_list_price_350       0\n",
      "plan_list_price_400       0\n",
      "plan_list_price_447       0\n",
      "plan_list_price_450       0\n",
      "plan_list_price_477       0\n",
      "plan_list_price_480       0\n",
      "plan_list_price_500       0\n",
      "plan_list_price_536       0\n",
      "plan_list_price_596       0\n",
      "plan_list_price_600       0\n",
      "plan_list_price_699       0\n",
      "plan_list_price_799       0\n",
      "plan_list_price_894       0\n",
      "plan_list_price_930       0\n",
      "plan_list_price_1000      0\n",
      "plan_list_price_1150      0\n",
      "plan_list_price_1200      0\n",
      "plan_list_price_1260      0\n",
      "plan_list_price_1299      0\n",
      "plan_list_price_1300      0\n",
      "plan_list_price_1399      0\n",
      "plan_list_price_1599      0\n",
      "plan_list_price_1788      0\n",
      "plan_list_price_2000      0\n",
      "Length: 149, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Get rid of null-values\n",
    "\n",
    "#For train data\n",
    "#Set city null values to o in train data\n",
    "cities = ['city_1','city_3','city_4','city_5','city_6','city_7','city_8','city_9','city_10','city_11','city_12','city_13','city_14','city_15','city_16','city_17','city_18','city_19','city_20','city_21','city_22']\n",
    "for i in range(0,len(cities)):\n",
    "        inpt = cities[i]\n",
    "        train_data[inpt]=train_data[inpt].fillna(value=0)\n",
    "        test_data[inpt]=test_data[inpt].fillna(value=0)\n",
    "\n",
    "user_logs_array = ['num_100', 'entries', 'num25_ratio', 'num50_ratio', 'num75_ratio', 'num985_ratio']\n",
    "#for i in range(0,len(user_logs_array)):\n",
    " #       inpt = user_logs_array[i]\n",
    "  #      train_data[inpt]=train_data[inpt].fillna(value=0)\n",
    "   #     test_data[inpt]=test_data[inpt].fillna(value=0)\n",
    "\n",
    "#Set registration dates null values to o in train data\n",
    "train_data[['reg_name','reg_month']] = train_data[['reg_name','reg_month']].fillna(value=0)\n",
    "test_data[['reg_name','reg_month']] = test_data[['reg_name','reg_month']].fillna(value=0)\n",
    "\n",
    "#check for null values\n",
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns (this may change with new insights or new code!)\n",
    "unwanted = ['msno','actual_amount_paid','transaction_date','membership_expire_date', 'diff_plan_actual','reg_name','reg_name']\n",
    "train_data = train_data.drop(unwanted, axis=1)\n",
    "test_data = test_data.drop(unwanted, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440062, 155)\n",
      "(440062, 149)\n"
     ]
    }
   ],
   "source": [
    "train_data_logs = train_data_logs.dropna()\n",
    "train_data_reduced = train_data_logs.drop(columns=['num_100', 'entries', 'num25_ratio', 'num50_ratio', 'num75_ratio', 'num985_ratio'],axis=1)\n",
    "print(train_data_logs.shape)\n",
    "print(train_data_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_logs = train_data_logs.drop(unwanted, axis=1)\n",
    "train_data_reduced = train_data_reduced.drop(unwanted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(970960, 142)\n",
      "(970960,)\n",
      "(440062, 148)\n",
      "(440062,)\n",
      "(440062, 142)\n",
      "(440062,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting input/output data into train and test sets in order to check efficiency of our models\n",
    "data_input = train_data.drop('is_churn',axis=1)\n",
    "data_output = train_data['is_churn']\n",
    "print(data_input.shape)\n",
    "print(data_output.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.2, random_state=42)\n",
    "\n",
    "# Removing is_churn (as it's all dummy zeros) from test data\n",
    "test_input = test_data.drop('is_churn',axis=1)\n",
    "\n",
    "# for user_logs data\n",
    "data_input_ul = train_data_logs.drop('is_churn',axis=1)\n",
    "data_output_ul = train_data_logs['is_churn']\n",
    "print(data_input_ul.shape)\n",
    "print(data_output_ul.shape)\n",
    "x_train_ul, x_test_ul, y_train_ul, y_test_ul = train_test_split(data_input_ul, data_output_ul, test_size=0.2, random_state=42)\n",
    "\n",
    "# for user_logs data\n",
    "data_input_r = train_data_reduced.drop('is_churn',axis=1)\n",
    "data_output_r = train_data_reduced['is_churn']\n",
    "print(data_input_r.shape)\n",
    "print(data_output_r.shape)\n",
    "x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(data_input_r, data_output_r, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Testing out prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e17d68932f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Random Forrest, no training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model fitted!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kkbox/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kkbox/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kkbox/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Random Forrest, no training data\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "print('Model fitted!')\n",
    "y_pred_f = model.predict(x_test)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Random Forrest is: %.2f\"%log_loss(y_test,y_pred_f))\n",
    "\n",
    "## I think we should use the logloss to measure our accuracy, as it is the same then they use on Kaggle. Also, to use accuracy, we need our input to be only 0's and 1's, thus it is not a very accurate assesment description of our model\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "#accuracy = accuracy_score(y_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted!\n",
      "Prediction done!\n",
      "Logloss for Random Forrest, incl. user_logs, is: 1.34\n",
      "Model fitted!\n",
      "Prediction done!\n",
      "Logloss for Random Forrest, user_logs size but no user_logs, is: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# user_logs included\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train_ul, y_train_ul)\n",
    "print('Model fitted!')\n",
    "y_pred_f = model.predict(x_test_ul)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Random Forrest, incl. user_logs, is: %.2f\"%log_loss(y_test_ul,y_pred_f))\n",
    "\n",
    "#vs.\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train_r, y_train_r)\n",
    "print('Model fitted!')\n",
    "y_pred_f = model.predict(x_test_r)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Random Forrest, user_logs size but no user_logs, is: %.2f\"%log_loss(y_test_r,y_pred_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted!\n",
      "Prediction done!\n",
      "Logloss for Linear Regression is: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "print('Model fitted!')\n",
    "# Make predicitons for test data\n",
    "y_pred_l = model.predict(x_test)\n",
    "y_pred_l = np.absolute(y_pred_l)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Linear Regression is: %.2f\"%log_loss(y_test,y_pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted!\n",
      "Prediction done!\n",
      "Logloss for Random Forrest, incl. user_logs, is: 0.34\n",
      "Model fitted!\n",
      "Prediction done!\n",
      "Logloss for Random Forrest, user_logs size but no user_logs, is: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# user_logs included\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_ul, y_train_ul)\n",
    "print('Model fitted!')\n",
    "y_pred_f = model.predict(x_test_ul)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Random Forrest, incl. user_logs, is: %.2f\"%log_loss(y_test_ul,y_pred_f))\n",
    "\n",
    "#vs.\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train_r, y_train_r)\n",
    "print('Model fitted!')\n",
    "y_pred_l = model.predict(x_test_r)\n",
    "print('Prediction done!')\n",
    "print(\"Logloss for Random Forrest, user_logs size but no user_logs, is: %.2f\"%log_loss(y_test_r,y_pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for AdaBoost is: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Run AdaBoost\n",
    "model_abr = AdaBoostRegressor()\n",
    "model_abr.fit(x_train, y_train)\n",
    "y_pred_abr = model_abr.predict(x_test)\n",
    "print(\"Logloss for AdaBoost is: %.2f\"%log_loss(y_test,y_pred_abr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for AdaBoost is: 27.72\n"
     ]
    }
   ],
   "source": [
    "# Run SGDRegressor (really bad)\n",
    "model_sgd = SGDRegressor()\n",
    "#model_sgd.fit(x_train, y_train)\n",
    "#y_pred_sgd = model_sgd.predict(x_test)\n",
    "print(\"Logloss for SGDRegressor is: %.2f\"%log_loss(y_test,y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for AdaBoost is: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Run GradientBoostingRegressor (very slow)\n",
    "model_gbr = GradientBoostingRegressor()\n",
    "#model_gbr.fit(x_train, y_train)\n",
    "#y_pred_gbr = model_gbr.predict(x_test)\n",
    "print(\"Logloss for GradientBoostingRegressor is: %.2f\"%log_loss(y_test,y_pred_gbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Boost\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_test, label = y_test)\n",
    "print('Done.')\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree. Try with max_depth: 2 to 10.\n",
    "    'eta': 0.3,  # the training step for each iteration. Try with ETA: 0.1, 0.2, 0.3...\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations. Try with num_round around few hundred!\n",
    "#----------------\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "print('Modeling done!')\n",
    "\n",
    "y_pred_xgb = bst.predict(dtest)\n",
    "print('Prediction done!')\n",
    "\n",
    "best_preds = np.asarray([np.argmax(line) for line in y_pred_xgb])\n",
    "\n",
    "y_pred_xgb = y_pred_xgb[:,1] #Column 2 out of 3\n",
    "\n",
    "print(\"Logloss for XGBoost is: %.3f\"%log_loss(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Running the best model on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model.fit(data_input, data_output)\n",
    "y_pred_final = model.predict(test_input)\n",
    "y_pred_final = np.absolute(y_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_churn                                          msno\n",
      "0  0.061539  4n+fXlyJvfQnTeKXTWT507Ll4JVYGrOC8LHCfwBmPE4=\n",
      "1  0.122253  aNmbC1GvFUxQyQUidCVmfbQ0YeCuwkPzEdQ0RwWyeZM=\n",
      "2  0.125351  rFC9eSG/tMuzpre6cwcMLZHEYM89xY02qcz7HL4//jc=\n",
      "3  0.058105  WZ59dLyrQcE7ft06MZ5dj40BnlYQY7PHgg/54+HaCSE=\n",
      "4  0.055695  aky/Iv8hMp1/V/yQHLtaVuEmmAxkB5GuasQZePJ7NU4=\n",
      "                                           msno  is_churn\n",
      "0  4n+fXlyJvfQnTeKXTWT507Ll4JVYGrOC8LHCfwBmPE4=  0.061539\n",
      "1  aNmbC1GvFUxQyQUidCVmfbQ0YeCuwkPzEdQ0RwWyeZM=  0.122253\n",
      "2  rFC9eSG/tMuzpre6cwcMLZHEYM89xY02qcz7HL4//jc=  0.125351\n",
      "3  WZ59dLyrQcE7ft06MZ5dj40BnlYQY7PHgg/54+HaCSE=  0.058105\n",
      "4  aky/Iv8hMp1/V/yQHLtaVuEmmAxkB5GuasQZePJ7NU4=  0.055695\n",
      "msno        907471\n",
      "is_churn    907471\n",
      "dtype: int64\n",
      "Done! :-)\n"
     ]
    }
   ],
   "source": [
    "#Prepare submission file\n",
    "my_submission = pd.DataFrame({'msno': test['msno'], 'is_churn': y_pred_final})\n",
    "print(my_submission.head())\n",
    "cols = my_submission.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "my_submission = my_submission[cols]\n",
    "print(my_submission.head())\n",
    "print(my_submission.count())\n",
    "\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "print('Done! :-)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
