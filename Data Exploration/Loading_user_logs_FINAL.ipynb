{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing the 30.41GB user_logs.csv File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the file's extraordinary size requires special measures to load, we created an notebook solely dedicated to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from collections import Counter\n",
    "from scipy.stats.stats import pearsonr # for pearson correlation\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "#import mpld3\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 3d visualization\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# computational time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a set of functions that we use to create additional features in the user_logs.csv and user_logs_v2.csv file. \n",
    "- grouping(dataframe): This function has two objectives. First, it will group the file by user msno's. This means, that, instead there being one row per listening session, we will have one row per user. The remaining features will be grouped by sum. Second, this function adds a new features called 'entries', which gives the amount of entries in the file for each user.\n",
    "- monthly_entries(dataframe,version): Here we create additional features that give us the amount of entries in the months Mar-17, Feb-17, Jan-17, Dec-16 and Nov-16. This allows us to to quantify changes in user activity in the months prior to month where we should predict churn.\n",
    "- monthly_secs(dataframe,version): Similar to the function above, we will sum total_secs for the months Nov-16 to Mar-17 to create new features.\n",
    "- days_since_function(dataframe,version): This function returns a dataframe with a new column called 'days_since' that tells us the how many days passed since the a user's last listening session and the last day of the month where we want to predict churn. While we believe this feature would add value and improve our prediction, we are currently having troubles to apply it on the whole user_logs.csv file due to it taking an immensely long time (approx. 400 hours). Therefore, all the rows related to days_since are #'d.\n",
    "- date_features(dataframe,version): This function actually applies the feature creation from the functions above to the user_logs files.\n",
    "- behaviour (dataframe): This function creates the new feature 'behaviour' and immediately applies it to the user_logs files. 'behaviour' is given by: (num_25 + ... + num_98.5)/num_100. Thus, 0 would mean that a user listens to every song entirely. We expect that users with changing listening behaviour will also have a changing likelihood to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by msno\n",
    "def grouping(dataframe):\n",
    "    counts = dataframe.groupby('msno')['num_25'].count().reset_index()\n",
    "    counts.columns = ['msno','entries']\n",
    "    sums = dataframe.groupby('msno').sum().reset_index()\n",
    "    dataframe = sums.merge(counts, how='inner', on='msno')\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "def monthly_entries(dataframe,version):\n",
    "    \n",
    "    idx_mar17 = (dataframe['date'] > datetime.date(2017,2,28)) & (dataframe['date'] < datetime.date(2017,4,1))\n",
    "    msno_counts = dataframe['msno'][idx_mar17].value_counts()\n",
    "    entries_mar17 = pd.DataFrame({'msno':msno_counts.index, 'entries_mar17':msno_counts.values})\n",
    "    \n",
    "    idx_feb17 = (dataframe['date'] > datetime.date(2017,1,31)) & (dataframe['date'] < datetime.date(2017,3,1))\n",
    "    msno_counts = dataframe['msno'][idx_feb17].value_counts()\n",
    "    entries_feb17 = pd.DataFrame({'msno':msno_counts.index, 'entries_feb17':msno_counts.values})\n",
    "    \n",
    "    idx_jan17 = (dataframe['date'] > datetime.date(2016,12,31)) & (dataframe['date'] < datetime.date(2017,2,1))\n",
    "    msno_counts = dataframe['msno'][idx_jan17].value_counts()\n",
    "    entries_jan17 = pd.DataFrame({'msno':msno_counts.index, 'entries_jan17':msno_counts.values})\n",
    "\n",
    "    idx_dec16 = (dataframe['date'] > datetime.date(2016,11,30)) & (dataframe['date'] < datetime.date(2017,1,1))\n",
    "    msno_counts = dataframe['msno'][idx_dec16].value_counts()\n",
    "    entries_dec16 = pd.DataFrame({'msno':msno_counts.index, 'entries_dec16':msno_counts.values})\n",
    "    \n",
    "    idx_nov16 = (dataframe['date'] > datetime.date(2016,10,31)) & (dataframe['date'] < datetime.date(2016,12,1))\n",
    "    msno_counts = dataframe['msno'][idx_nov16].value_counts()\n",
    "    entries_nov16 = pd.DataFrame({'msno':msno_counts.index, 'entries_nov16':msno_counts.values})\n",
    "    \n",
    "    if (version == 1):    \n",
    "        monthly_entries_df = entries_nov16.merge(entries_dec16,on='msno',how='outer')\n",
    "        monthly_entries_df = monthly_entries_df.merge(entries_jan17,on='msno',how='outer')\n",
    "        monthly_entries_df = monthly_entries_df.merge(entries_feb17,on='msno',how='outer')\n",
    "    elif (version == 2):\n",
    "        monthly_entries_df = entries_mar17\n",
    "        \n",
    "    return monthly_entries_df\n",
    "\n",
    "def monthly_secs(dataframe,version):\n",
    "    \n",
    "    idx_mar17 = (dataframe['date'] > datetime.date(2017,2,28)) & (dataframe['date'] < datetime.date(2017,4,1))\n",
    "    secs_mar17 = user_logs_datef_secs[idx_mar17].groupby('msno').sum().reset_index()\n",
    "    secs_mar17 = secs_mar17.rename(index=str, columns={\"total_secs\": \"secs_mar17\"})\n",
    "    \n",
    "    idx_feb17 = (dataframe['date'] > datetime.date(2017,1,31)) & (dataframe['date'] < datetime.date(2017,3,1))\n",
    "    secs_feb17 = user_logs_datef_secs[idx_feb17].groupby('msno').sum().reset_index()\n",
    "    secs_feb17 = secs_feb17.rename(index=str, columns={\"total_secs\": \"secs_feb17\"})\n",
    "    \n",
    "    idx_jan17 = (dataframe['date'] > datetime.date(2016,12,31)) & (dataframe['date'] < datetime.date(2017,2,1))\n",
    "    secs_jan17 = user_logs_datef_secs[idx_jan17].groupby('msno').sum().reset_index()\n",
    "    secs_jan17 = secs_jan17.rename(index=str, columns={\"total_secs\": \"secs_jan17\"})\n",
    "    \n",
    "    idx_dec16 = (dataframe['date'] > datetime.date(2016,11,30)) & (dataframe['date'] < datetime.date(2017,1,1))\n",
    "    secs_dec16 = user_logs_datef_secs[idx_dec16].groupby('msno').sum().reset_index()\n",
    "    secs_dec16 = secs_dec16.rename(index=str, columns={\"total_secs\": \"secs_dec16\"})\n",
    "    \n",
    "    idx_nov16 = (dataframe['date'] > datetime.date(2016,10,31)) & (dataframe['date'] < datetime.date(2016,12,1))\n",
    "    secs_nov16 = user_logs_datef_secs[idx_nov16].groupby('msno').sum().reset_index()\n",
    "    secs_nov16 = secs_nov16.rename(index=str, columns={\"total_secs\": \"secs_nov16\"})\n",
    "    \n",
    "    if (version == 1):\n",
    "        monthly_secs_df = secs_dec16.merge(secs_nov16,on='msno',how='outer')\n",
    "        monthly_secs_df = monthly_secs_df.merge(secs_jan17,on='msno',how='outer')\n",
    "        monthly_secs_df = monthly_secs_df.merge(secs_feb17,on='msno',how='outer')\n",
    "    elif (version == 2):\n",
    "        monthly_secs_df = secs_mar17\n",
    "            \n",
    "    return monthly_secs_df\n",
    "\n",
    "# days since last session (from Mar 2017, but needs to be put to Apr 2017 for test !!!!)\n",
    "def days_since_function(dataframe,version):\n",
    "    \n",
    "    days_since = dataframe.groupby('msno', as_index=False)['date'].max()\n",
    "    days_since['days_since_mar'] = 0\n",
    "    days_since['days_since_apr'] = 0\n",
    "    if (version == 1):\n",
    "        days_since['days_since_mar'] = datetime.date(2017,3,31) - days_since['date'] \n",
    "        days_since['days_since_mar'] = days_since['days_since_mar'].dt.days\n",
    "    elif (version == 2):\n",
    "        days_since['days_since_apr'] = datetime.date(2017,4,30) - days_since['date'] \n",
    "        days_since['days_since_apr'] = days_since['days_since_apr'].dt.days\n",
    "    \n",
    "    return days_since\n",
    "\n",
    "def date_features(dataframe,version):\n",
    "    monthly_entries_df = monthly_entries(dataframe,version)\n",
    "    monthly_secs_df = monthly_secs(dataframe,version)\n",
    "    #days_since = days_since_function(dataframe,version)\n",
    "    user_logs_grouped = grouping(dataframe)\n",
    "    user_logs_eng = user_logs_grouped.merge(monthly_entries_df,on='msno',how='outer')\n",
    "    user_logs_eng = user_logs_eng.merge(monthly_secs_df,on='msno',how='outer')\n",
    "    #user_logs_eng = user_logs_eng.merge(days_since,on='msno',how='outer')\n",
    "    \n",
    "    return user_logs_eng\n",
    "\n",
    "def behaviour (dataframe):\n",
    "    num_columns = ['num_25','num_50','num_75','num_985','num_100']\n",
    "    sum = 0\n",
    "    for i in range(0,len(num_columns)-1):\n",
    "        inpt = num_columns[i]\n",
    "        sum = sum + dataframe[inpt]\n",
    "    dataframe['behaviour'] = sum/dataframe['num_100'][dataframe['num_100']!=0]\n",
    "    dataframe['behaviour'] = dataframe['behaviour'].fillna(value=1)\n",
    "    \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Files\n",
    "### 2.1 user_logs.csv by chunks of 10e8 bytes\n",
    "\n",
    "The user_logs.csv file has 30.51 GB and thus, due to memory restrictions, cannot be read in by normal computers in one go. Therefore, we use the parameter 'chunksize' from pandas' read_csv function to read in the file by chunks of 10^8 bytes and append them iteratively. While features like 'behaviour' can be derived from a grouped-by-msno file, which will be significantly smaller than the original file(around 400 times smaller), some of our features are derived from the whole dataset and thus need to be generated during the loop. This, in particular, includes all date-related features, as the group by msno will remove dates. Hence, all the features given by the functions monthly_entries and monthly_secs will have be generated in the loop. \n",
    "While we already group the chunks by msno in the loop, we need to do it again after the loop as msno's may appear in several different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Time = 9628.721282720566\n"
     ]
    }
   ],
   "source": [
    "# actual solution\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "chunksize = 10 ** 8\n",
    "\n",
    "# initialize user_logs_eng_v1 with only columns names for the loop\n",
    "user_logs_eng_v1 = pd.read_csv('data/user_logs_20m.csv', nrows=0)\n",
    "user_logs_eng_v1 = user_logs_eng_v1.drop('date',axis=1)\n",
    "user_logs_eng_v1 = grouping(user_logs_eng_v1)\n",
    "\n",
    "for chunk in pd.read_csv('/Volumes/TOSHIBA EXT/Files/user_logs.csv', chunksize=chunksize):\n",
    "    \n",
    "    # Changing date types\n",
    "    chunk['date'] = chunk.date.apply(lambda x: dt.strptime(str(int(x)), \"%Y%m%d\").date() if pd.notnull(x) else \"NAN\" )\n",
    "\n",
    "    # date features\n",
    "    chunk = date_features(chunk,1)\n",
    "        \n",
    "    # append\n",
    "    user_logs_eng_v1 = user_logs_eng_v1.append(chunk)\n",
    "\n",
    "# group again the msno's, as they may appear in more than one chunk\n",
    "# days_since = user_logs_eng_v1.groupby('msno', as_index=False)['day_since'].max()\n",
    "user_logs_eng_v1 = user_logs_eng_v1.groupby('msno').sum().reset_index()\n",
    "# user_logs_eng_v1['days_since'] = days_since['days_since']\n",
    "\n",
    "# adding the 'behaviour' column\n",
    "user_logs_eng_v1 = behaviour(user_logs_eng_v1)\n",
    "\n",
    "# make this part immortal\n",
    "user_logs_eng_immortal2 = user_logs_eng_v1.copy()\n",
    "user_logs_eng_immortal2.to_csv('data/user_logs_eng_immortal.csv', index=False)\n",
    "print(\"Done!\")\n",
    "\n",
    "print('Time =',time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading user_logs_v2.csv file\n",
    "\n",
    "User_logs_v2.csv is the same dataset than user_logs.csv, except that it only has entries from Mar-17. In contrast user_logs.csv has entries from 2015 to Feb-17. Thus the only features that we create in user_logs_v2.csv are 'behaviour' and entries and total_secs for Mar-17. As the file is only 1.43 GB, we don't need to read it in in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18396362, 9)\n",
      "unique msnos in the dataset:  1103894\n",
      "rows in the new one:  1103894\n",
      "(1103894, 12)\n",
      "Done!\n",
      "Time = 425.85778403282166\n",
      "msno             0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_100          0\n",
      "num_unq          0\n",
      "total_secs       0\n",
      "entries          0\n",
      "entries_mar17    0\n",
      "secs_mar17       0\n",
      "behaviour        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# loading user_logs_v2\n",
    "user_logs_v2 = pd.read_csv('data/user_logs_v2.csv')\n",
    "print(user_logs_v2.shape)\n",
    "print('unique msnos in the dataset: ',user_logs_v2['msno'].nunique())\n",
    "\n",
    "# Changing date types\n",
    "user_logs_v2['date'] = user_logs_v2.date.apply(lambda x: dt.strptime(str(int(x)), \"%Y%m%d\").date() if pd.notnull(x) else \"NAN\" )\n",
    "\n",
    "# date features\n",
    "user_logs_v2 = date_features(user_logs_v2,2)\n",
    "print('rows in the new one: ',user_logs_v2.shape[0])\n",
    "\n",
    "# adding the 'behaviour' column\n",
    "user_logs_v2 = behaviour(user_logs_v2)\n",
    "\n",
    "# group to remove NaN values\n",
    "user_logs_v2 = user_logs_v2.groupby('msno').sum().reset_index()\n",
    "\n",
    "# removing v1 parts\n",
    "#user_logs_v2 = user_logs_v2[['msno','entries_mar17','secs_mar17']]\n",
    "print(user_logs_v2.shape)\n",
    "\n",
    "user_logs_v2_immortal = user_logs_v2.copy()\n",
    "user_logs_v2_immortal.to_csv('data/user_logs_v2_immortal.csv', index=False)\n",
    "\n",
    "print(\"Done!\")\n",
    "print('Time =',time.time() - start)\n",
    "\n",
    "print(user_logs_v2.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           msno  num_25  num_50  num_75  \\\n",
      "0  +++IZseRRiQS9aaSkH6cMYU6bGDcxUieAi/tH67sC5s=      86      11      10   \n",
      "1  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=     191      90      75   \n",
      "2  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=      43      12      15   \n",
      "3  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=     207     163     100   \n",
      "4  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=     105      24      39   \n",
      "\n",
      "   num_985  num_100  num_unq  total_secs  entries  entries_mar17  secs_mar17  \\\n",
      "0        5      472      530  117907.425       26             26  117907.425   \n",
      "1      144      589      885  192527.892       31             31  192527.892   \n",
      "2       12      485      468  115411.260       28             28  115411.260   \n",
      "3       64      436      828  149896.558       21             21  149896.558   \n",
      "4       35      479      230  116433.247       29             29  116433.247   \n",
      "\n",
      "   behaviour  \n",
      "0   0.237288  \n",
      "1   0.848896  \n",
      "2   0.169072  \n",
      "3   1.224771  \n",
      "4   0.423800  \n"
     ]
    }
   ],
   "source": [
    "print(user_logs_v2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs_v1 = pd.read_csv('data/user_logs_eng_immortal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two lines below only had to be implemented because I had a typo in the tab where I load the 30GB file\n",
    "# in order to not load it again I corrected it below\n",
    "user_logs_v1['entries_nov16'] = user_logs_v1['entries_nov16'] - user_logs_v1['entries_dec16'] - user_logs_v1['entries_jan17'] - user_logs_v1['entries_feb17']\n",
    "user_logs_v1['secs_nov16'] = user_logs_v1['secs_nov16'] - user_logs_v1['secs_dec16'] - user_logs_v1['secs_jan17'] - user_logs_v1['secs_feb17']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs_v2 = pd.read_csv('data/user_logs_v2_immortal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_logs_check = pd.read_csv('data/user_logs_20m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_logs_check[user_logs_check['msno'] == '+++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loading Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train files\n",
    "train1 = pd.read_csv('data/train.csv')\n",
    "train2 = pd.read_csv('data/train_v2.csv')\n",
    "submission = pd.read_csv('data/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Removing negative total_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had previously seen in the Data Exploration part that there are some entries with negative total_secs. We simply put those entries to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    5.234111e+06\n",
      "mean     6.033205e+10\n",
      "std      2.352035e+13\n",
      "min      0.000000e+00\n",
      "25%      1.025253e+03\n",
      "50%      7.664507e+03\n",
      "75%      1.230398e+05\n",
      "max      9.223372e+15\n",
      "Name: total_secs, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "user_logs_v1['total_secs'][user_logs_v1['total_secs'] < 0] = 0\n",
    "user_logs_v1['secs_nov16'][user_logs_v1['secs_nov16'] < 0] = 0\n",
    "user_logs_v1['secs_dec16'][user_logs_v1['secs_dec16'] < 0] = 0\n",
    "user_logs_v1['secs_jan17'][user_logs_v1['secs_jan17'] < 0] = 0\n",
    "user_logs_v1['secs_feb17'][user_logs_v1['secs_feb17'] < 0] = 0\n",
    "\n",
    "user_logs_v2['total_secs'][user_logs_v2['total_secs'] < 0] = 0\n",
    "user_logs_v2['secs_mar17'][user_logs_v2['secs_mar17'] < 0] = 0\n",
    "\n",
    "print(user_logs_v1['total_secs'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             num_25        num_50        num_75       num_985       num_100  \\\n",
      "count  1.103894e+06  1.103894e+06  1.103894e+06  1.103894e+06  1.103894e+06   \n",
      "mean   1.031795e+02  2.514392e+01  1.568800e+01  1.799658e+01  5.046563e+02   \n",
      "std    1.722185e+02  3.856735e+01  2.221605e+01  6.143512e+01  7.606139e+02   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.500000e+01  4.000000e+00  2.000000e+00  2.000000e+00  7.500000e+01   \n",
      "50%    5.000000e+01  1.300000e+01  9.000000e+00  9.000000e+00  2.710000e+02   \n",
      "75%    1.260000e+02  3.200000e+01  2.100000e+01  2.200000e+01  6.280000e+02   \n",
      "max    4.682000e+04  3.163000e+03  1.690000e+03  1.516400e+04  5.767500e+04   \n",
      "\n",
      "            num_unq    total_secs       entries  entries_mar17    secs_mar17  \\\n",
      "count  1.103894e+06  1.103894e+06  1.103894e+06   1.103894e+06  1.103894e+06   \n",
      "mean   4.838865e+02  1.317335e+05  1.666497e+01   1.666497e+01  1.317335e+05   \n",
      "std    5.952788e+02  1.852267e+05  1.030333e+01   1.030333e+01  1.852267e+05   \n",
      "min    1.000000e+00  2.000000e-03  1.000000e+00   1.000000e+00  2.000000e-03   \n",
      "25%    9.000000e+01  2.133471e+04  7.000000e+00   7.000000e+00  2.133471e+04   \n",
      "50%    2.970000e+02  7.382801e+04  1.800000e+01   1.800000e+01  7.382801e+04   \n",
      "75%    6.530000e+02  1.675735e+05  2.600000e+01   2.600000e+01  1.675735e+05   \n",
      "max    2.340300e+04  1.433739e+07  3.100000e+01   3.100000e+01  1.433739e+07   \n",
      "\n",
      "          behaviour  \n",
      "count  1.103894e+06  \n",
      "mean   8.258167e-01  \n",
      "std    4.499647e+00  \n",
      "min    0.000000e+00  \n",
      "25%    1.484375e-01  \n",
      "50%    3.389021e-01  \n",
      "75%    7.080745e-01  \n",
      "max    2.459500e+03  \n"
     ]
    }
   ],
   "source": [
    "print(user_logs_v2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Processing\n",
    "We will now create 3 different files. We currently have three different is_churn dates we have to look at. Train.csv gives us is_churn in Feb-17, train_v2.csv in Mar-17 and for the submission we are required to predict Apr-17. As our most up-to-date data is from Mar-17, we can always just use data from the previous month to predict is_churn.\n",
    "\n",
    "(THIS IS REALLY IMPORTANT TO UNDERSTAND SO I EXPLAIN IT AGAIN)\n",
    "\n",
    "We are given a dataset with user data that includes user listening session and user transactions from 2015 to Mar-17. We are supposed to use this data to predict is_churn for Apr-17. This means that we must predict is_churn for the month after the month we have our most recent data. \n",
    "\n",
    "As we must train our models under exactly the same conditions than we 'test' them on (meaning we create predictions on), we have to make sure that the data we use to train the model is also from the month before the is_churn.\n",
    "\n",
    "Thus, as we use is_churn from Mar-17 (train_v2.csv) for our train model, we have to use data from 2015 to Feb-17 as the input. \n",
    "\n",
    "For testing our prediction without uploading it to Kaggle, we can use is_churn from Feb-17 (train.csv) to train and is_churn from Mar-17 to check our prediction. Thus, for the train model we will data from 2015 to Jan-17. This means that we will have to manually remove all data after Jan-17 from the user_logs.csv and transactions.csv.\n",
    "\n",
    "- One with is_churn data from Feb-17 and user_logs data from 2015 to Jan-17.\n",
    "- One with is_churn data from Mar-17 and user_logs data from 2015 to Feb-17.\n",
    "- One which will be used to predict is_churn in Apr-17 and with user_logs data from 2015 to Mar-17.\n",
    "\n",
    "### 3.1 Merging with Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST NOTICED: FOR VERSION 1 I ALSO NEED TO NOT ADD ABSOLUTE VALUES FROM THE LAST MONTHS #\n",
    "\n",
    "# first dataset: is_churn from train1\n",
    "# we drop entries and secs from February 17 due to the reasons given in the explanation above\n",
    "user_logs_1 = user_logs_v1.drop(columns=['entries_feb17','secs_feb17'],axis=1)\n",
    "user_logs_1 = user_logs_1.merge(train1,on='msno',how='right')\n",
    "\n",
    "# second dataset: is_churn from train2\n",
    "user_logs_2 = user_logs_v1.merge(train2,on='msno',how='right')\n",
    "\n",
    "# third dataset\n",
    "# as user_logs_v2 only contains data from Mar-17 we have to merge it with user_logs_v1 to add the data from 2015 to Feb-17.\n",
    "user_logs_3 = user_logs_v2.merge(user_logs_v1[['msno','secs_dec16','secs_feb17','secs_jan17','secs_nov16','entries_nov16','entries_dec16','entries_feb17','entries_jan17']],on='msno',how='outer')\n",
    "user_logs_3 = user_logs_3.merge(submission,on='msno',how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 2)\n",
      "(970960, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train1.shape)\n",
    "print(train2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 17)\n",
      "(970960, 19)\n",
      "(907471, 21)\n",
      "                                           msno  entries  entries_dec16  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=    103.0           31.0   \n",
      "1  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=    609.0           31.0   \n",
      "2  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=    603.0           24.0   \n",
      "3  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=    292.0           26.0   \n",
      "4  ++/UDNo9DLrxT8QVGiDi1OnWfczAdEwThaVyD0fXO50=    445.0           18.0   \n",
      "\n",
      "   entries_jan17  entries_nov16  num_100  num_25  num_50  num_75  num_985  \\\n",
      "0           30.0           15.0   2231.0   572.0   289.0   284.0    504.0   \n",
      "1           31.0           29.0  15572.0  1396.0   494.0   420.0    561.0   \n",
      "2           27.0           17.0  23807.0  6688.0  2213.0  1476.0   1402.0   \n",
      "3           27.0           26.0   5092.0  1438.0   449.0   350.0    471.0   \n",
      "4           27.0           19.0   4803.0  1083.0   540.0   299.0    415.0   \n",
      "\n",
      "   num_unq  secs_dec16  secs_jan17  secs_nov16   total_secs  behaviour  \\\n",
      "0   2805.0  256812.004  157868.677  148668.073   719882.711   0.739130   \n",
      "1  15277.0  154978.397  198105.280  133337.861  4061562.584   0.184369   \n",
      "2  24664.0  169086.593  215757.136  289423.141  6683044.760   0.494770   \n",
      "3   3864.0  132765.529  106043.142  129382.878  1485801.348   0.531815   \n",
      "4   6635.0   54255.659   51758.313   29430.154  1398801.748   0.486571   \n",
      "\n",
      "   is_churn  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "                                           msno  entries  entries_dec16  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=    103.0           31.0   \n",
      "1  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=    609.0           31.0   \n",
      "2  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=    603.0           24.0   \n",
      "3  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=    292.0           26.0   \n",
      "4  ++/UDNo9DLrxT8QVGiDi1OnWfczAdEwThaVyD0fXO50=    445.0           18.0   \n",
      "\n",
      "   entries_feb17  entries_jan17  entries_nov16  num_100  num_25  num_50  \\\n",
      "0           27.0           30.0           15.0   2231.0   572.0   289.0   \n",
      "1           28.0           31.0           29.0  15572.0  1396.0   494.0   \n",
      "2           20.0           27.0           17.0  23807.0  6688.0  2213.0   \n",
      "3           25.0           27.0           26.0   5092.0  1438.0   449.0   \n",
      "4           18.0           27.0           19.0   4803.0  1083.0   540.0   \n",
      "\n",
      "   num_75  num_985  num_unq  secs_dec16  secs_feb17  secs_jan17  secs_nov16  \\\n",
      "0   284.0    504.0   2805.0  256812.004  156533.957  157868.677  148668.073   \n",
      "1   420.0    561.0  15277.0  154978.397  189007.847  198105.280  133337.861   \n",
      "2  1476.0   1402.0  24664.0  169086.593  123768.839  215757.136  289423.141   \n",
      "3   350.0    471.0   3864.0  132765.529   78863.188  106043.142  129382.878   \n",
      "4   299.0    415.0   6635.0   54255.659   49219.147   51758.313   29430.154   \n",
      "\n",
      "    total_secs  behaviour  is_churn  \n",
      "0   719882.711   0.739130         0  \n",
      "1  4061562.584   0.184369         0  \n",
      "2  6683044.760   0.494770         0  \n",
      "3  1485801.348   0.531815         0  \n",
      "4  1398801.748   0.486571         0  \n",
      "                                           msno  num_25  num_50  num_75  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=   191.0    90.0    75.0   \n",
      "1  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=   207.0   163.0   100.0   \n",
      "2  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=   105.0    24.0    39.0   \n",
      "3  ++0/NopttBsaAn6qHZA2AWWrDg7Me7UOMs1vsyo4tSI=    21.0     8.0    17.0   \n",
      "4  ++0BJXY8tpirgIhJR14LDM1pnaRosjD1mdO1mIKxlJA=    27.0    15.0    10.0   \n",
      "\n",
      "   num_985  num_100  num_unq  total_secs  entries  entries_mar17    ...     \\\n",
      "0    144.0    589.0    885.0  192527.892     31.0           31.0    ...      \n",
      "1     64.0    436.0    828.0  149896.558     21.0           21.0    ...      \n",
      "2     35.0    479.0    230.0  116433.247     29.0           29.0    ...      \n",
      "3      7.0    104.0    115.0   28450.268      8.0            8.0    ...      \n",
      "4      4.0    115.0    163.0   31788.296      9.0            9.0    ...      \n",
      "\n",
      "   behaviour  secs_dec16  secs_feb17  secs_jan17  secs_nov16  entries_nov16  \\\n",
      "0   0.848896  256812.004  156533.957  157868.677  148668.073           15.0   \n",
      "1   1.224771  169086.593  123768.839  215757.136  289423.141           17.0   \n",
      "2   0.423800  132765.529   78863.188  106043.142  129382.878           26.0   \n",
      "3   0.509615   33137.642    5712.573   22640.344    7518.001            6.0   \n",
      "4   0.486957   41222.972   23387.903   15738.694    9741.237            7.0   \n",
      "\n",
      "   entries_dec16  entries_feb17  entries_jan17  is_churn  \n",
      "0           31.0           27.0           30.0         0  \n",
      "1           24.0           20.0           27.0         0  \n",
      "2           26.0           25.0           27.0         0  \n",
      "3           13.0            4.0            7.0         0  \n",
      "4            7.0            5.0            9.0         0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_1.shape)\n",
    "print(user_logs_2.shape)\n",
    "print(user_logs_3.shape)\n",
    "\n",
    "print(user_logs_1.head())\n",
    "print(user_logs_2.head())\n",
    "print(user_logs_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             entries  entries_dec16  entries_jan17  entries_nov16  \\\n",
      "count  869926.000000  869926.000000  869926.000000  869926.000000   \n",
      "mean      282.261706      15.383880      15.649394      14.380819   \n",
      "std       224.093097      10.928904      10.408171      10.781273   \n",
      "min         1.000000       0.000000       0.000000       0.000000   \n",
      "25%        83.000000       5.000000       6.000000       3.000000   \n",
      "50%       234.000000      16.000000      16.000000      15.000000   \n",
      "75%       453.000000      26.000000      25.000000      25.000000   \n",
      "max       790.000000      31.000000      31.000000      30.000000   \n",
      "\n",
      "             num_100         num_25         num_50         num_75  \\\n",
      "count  869926.000000  869926.000000  869926.000000  869926.000000   \n",
      "mean     8644.731970    1836.855691     456.326159     285.663154   \n",
      "std     12682.459724    2940.346760     635.687247     386.645811   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%      1225.000000     279.000000      76.000000      49.000000   \n",
      "50%      4494.000000     901.000000     244.000000     158.000000   \n",
      "75%     11217.000000    2294.000000     594.000000     379.000000   \n",
      "max    444898.000000  911417.000000   40823.000000   37596.000000   \n",
      "\n",
      "             num_985        num_unq    secs_dec16    secs_jan17    secs_nov16  \\\n",
      "count  869926.000000  869926.000000  8.699260e+05  8.699260e+05  8.699260e+05   \n",
      "mean      317.304351    8497.097622  1.218553e+05  1.240024e+05  1.146593e+05   \n",
      "std       524.871776   10758.822564  1.770172e+05  1.711562e+05  1.712594e+05   \n",
      "min         0.000000       1.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%        48.000000    1421.000000  1.165545e+04  1.786824e+04  7.259686e+03   \n",
      "50%       161.000000    4865.000000  6.498398e+04  7.102008e+04  5.790810e+04   \n",
      "75%       399.000000   11648.750000  1.592204e+05  1.618024e+05  1.502410e+05   \n",
      "max     56441.000000  268047.000000  1.251004e+07  7.181395e+06  8.961098e+06   \n",
      "\n",
      "         total_secs      behaviour       is_churn  \n",
      "count  8.699260e+05  869926.000000  992931.000000  \n",
      "mean   1.383770e+11       0.775493       0.063923  \n",
      "std    3.565833e+13       3.328018       0.244616  \n",
      "min    0.000000e+00       0.000000       0.000000  \n",
      "25%    3.006926e+05       0.208476       0.000000  \n",
      "50%    1.145818e+06       0.387097       0.000000  \n",
      "75%    2.880615e+06       0.702033       0.000000  \n",
      "max    9.223372e+15     806.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(user_logs_1.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we created the new features, some entries ended up being Null values. For features that contain the number of entries, seconds or songs, we set Null to 0. For 'behaviour', we set Null to 1, as 0 is not neutral enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno                  0\n",
      "entries          123005\n",
      "entries_dec16    123005\n",
      "entries_jan17    123005\n",
      "entries_nov16    123005\n",
      "num_100          123005\n",
      "num_25           123005\n",
      "num_50           123005\n",
      "num_75           123005\n",
      "num_985          123005\n",
      "num_unq          123005\n",
      "secs_dec16       123005\n",
      "secs_jan17       123005\n",
      "secs_nov16       123005\n",
      "total_secs       123005\n",
      "behaviour        123005\n",
      "is_churn              0\n",
      "dtype: int64\n",
      "msno                  0\n",
      "entries          120664\n",
      "entries_dec16    120664\n",
      "entries_feb17    120664\n",
      "entries_jan17    120664\n",
      "entries_nov16    120664\n",
      "num_100          120664\n",
      "num_25           120664\n",
      "num_50           120664\n",
      "num_75           120664\n",
      "num_985          120664\n",
      "num_unq          120664\n",
      "secs_dec16       120664\n",
      "secs_feb17       120664\n",
      "secs_jan17       120664\n",
      "secs_nov16       120664\n",
      "total_secs       120664\n",
      "behaviour        120664\n",
      "is_churn              0\n",
      "dtype: int64\n",
      "msno                  0\n",
      "num_25           207242\n",
      "num_50           207242\n",
      "num_75           207242\n",
      "num_985          207242\n",
      "num_100          207242\n",
      "num_unq          207242\n",
      "total_secs       207242\n",
      "entries          207242\n",
      "entries_mar17    207242\n",
      "secs_mar17       207242\n",
      "behaviour        207242\n",
      "secs_dec16       141049\n",
      "secs_feb17       141049\n",
      "secs_jan17       141049\n",
      "secs_nov16       141049\n",
      "entries_nov16    141049\n",
      "entries_dec16    141049\n",
      "entries_feb17    141049\n",
      "entries_jan17    141049\n",
      "is_churn              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_1.isnull().sum())\n",
    "print(user_logs_2.isnull().sum())\n",
    "print(user_logs_3.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_to_0_columns = ['secs_dec16','secs_feb17','secs_jan17','entries_dec16','entries_feb17','entries_jan17',\n",
    "                     'entries_mar17','entries_nov16','secs_mar17','secs_nov16','entries','num_25','num_50','num_75','num_985','num_100',\n",
    "                     'num_unq','total_secs']\n",
    "\n",
    "for i in range(0,len(Null_to_0_columns)):\n",
    "    inpt = Null_to_0_columns[i]\n",
    "    if (inpt in user_logs_1.columns):\n",
    "        user_logs_1[inpt] = user_logs_1[inpt].fillna(value=0)\n",
    "    if (inpt in user_logs_2.columns):\n",
    "        user_logs_2[inpt] = user_logs_2[inpt].fillna(value=0)\n",
    "    if (inpt in user_logs_3.columns):\n",
    "        user_logs_3[inpt] = user_logs_3[inpt].fillna(value=0)\n",
    " \n",
    "user_logs_1['behaviour'] = user_logs_1['behaviour'].fillna(value=1)\n",
    "user_logs_2['behaviour'] = user_logs_2['behaviour'].fillna(value=1)\n",
    "user_logs_3['behaviour'] = user_logs_3['behaviour'].fillna(value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno             0\n",
      "entries          0\n",
      "entries_dec16    0\n",
      "entries_jan17    0\n",
      "entries_nov16    0\n",
      "num_100          0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_unq          0\n",
      "secs_dec16       0\n",
      "secs_jan17       0\n",
      "secs_nov16       0\n",
      "total_secs       0\n",
      "behaviour        0\n",
      "is_churn         0\n",
      "dtype: int64\n",
      "msno             0\n",
      "entries          0\n",
      "entries_dec16    0\n",
      "entries_feb17    0\n",
      "entries_jan17    0\n",
      "entries_nov16    0\n",
      "num_100          0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_unq          0\n",
      "secs_dec16       0\n",
      "secs_feb17       0\n",
      "secs_jan17       0\n",
      "secs_nov16       0\n",
      "total_secs       0\n",
      "behaviour        0\n",
      "is_churn         0\n",
      "dtype: int64\n",
      "msno             0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_100          0\n",
      "num_unq          0\n",
      "total_secs       0\n",
      "entries          0\n",
      "entries_mar17    0\n",
      "secs_mar17       0\n",
      "behaviour        0\n",
      "secs_dec16       0\n",
      "secs_feb17       0\n",
      "secs_jan17       0\n",
      "secs_nov16       0\n",
      "entries_nov16    0\n",
      "entries_dec16    0\n",
      "entries_feb17    0\n",
      "entries_jan17    0\n",
      "is_churn         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_1.isnull().sum())\n",
    "print(user_logs_2.isnull().sum())\n",
    "print(user_logs_3.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a bunch of ratios in order to to quantify trends in user behaviour. These ratios are: \n",
    "- cVp_entries = current month entries / previous month entries (where current always depicts the month before is_churn is given) (e.g. if is_churn is given for Feb-17, current month will be January)\n",
    "- pVpp_entries = previous month / previous previous month (FOR LATER!)\n",
    "- cVp_ and pVpp_secs = same logic than above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 17)\n",
      "(970960, 19)\n",
      "(907471, 21)\n"
     ]
    }
   ],
   "source": [
    "print(user_logs_1.shape)\n",
    "print(user_logs_2.shape)\n",
    "print(user_logs_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/maximekayser/miniconda3/envs/kkbox/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# is_churn in Feb\n",
    "ul_1 = user_logs_1.copy()\n",
    "\n",
    "ul_1['cVp_entries'] = 0\n",
    "# avoid NaN or inf values when denominator is 0\n",
    "idx_1 = (ul_1['entries_dec16'] == 0) \n",
    "ul_1['cVp_entries'][idx_1] = ul_1['entries_jan17'][idx_1]\n",
    "ul_1['cVp_entries'][~idx_1] = ul_1['entries_jan17'][~idx_1]/ul_1['entries_dec16'][~idx_1]\n",
    "\n",
    "ul_1['pVpp_entries'] = 0\n",
    "# avoid NaN or inf values when denominator is 0\n",
    "idx_1 = (ul_1['entries_nov16'] == 0) \n",
    "ul_1['pVpp_entries'][idx_1] = ul_1['entries_dec16'][idx_1]\n",
    "ul_1['pVpp_entries'][~idx_1] = ul_1['entries_dec16'][~idx_1]/ul_1['entries_nov16'][~idx_1]\n",
    "\n",
    "ul_1['cVp_secs'] = 0\n",
    "# avoid NaN or inf values when denominator is 0\n",
    "idx_1 = (ul_1['secs_dec16'] == 0) \n",
    "ul_1['cVp_secs'][idx_1] = ul_1['secs_jan17'][idx_1]\n",
    "ul_1['cVp_secs'][~idx_1] = ul_1['secs_jan17'][~idx_1]/ul_1['secs_dec16'][~idx_1]\n",
    "\n",
    "ul_1['pVpp_secs'] = 0\n",
    "# avoid NaN or inf values when denominator is 0\n",
    "idx_1 = (ul_1['secs_nov16'] == 0) \n",
    "ul_1['pVpp_secs'][idx_1] = ul_1['secs_dec16'][idx_1]\n",
    "ul_1['pVpp_secs'][~idx_1] = ul_1['secs_dec16'][~idx_1]/ul_1['secs_nov16'][~idx_1]\n",
    "         \n",
    "# is_churn in Mar\n",
    "ul_2 = user_logs_2.copy()\n",
    "         \n",
    "ul_2['cVp_entries'] = 0\n",
    "idx_1 = (ul_2['entries_jan17'] == 0) \n",
    "ul_2['cVp_entries'][idx_1] = ul_2['entries_feb17'][idx_1]\n",
    "ul_2['cVp_entries'][~idx_1] = ul_2['entries_feb17'][~idx_1]/ul_2['entries_jan17'][~idx_1]\n",
    "\n",
    "ul_2['pVpp_entries'] = 0\n",
    "idx_1 = (ul_2['entries_dec16'] == 0) \n",
    "ul_2['pVpp_entries'][idx_1] = ul_2['entries_jan17'][idx_1]\n",
    "ul_2['pVpp_entries'][~idx_1] = ul_2['entries_jan17'][~idx_1]/ul_2['entries_dec16'][~idx_1]\n",
    "  \n",
    "ul_2['cVp_secs'] = 0\n",
    "idx_1 = (ul_2['secs_jan17'] == 0) \n",
    "ul_2['cVp_secs'][idx_1] = ul_2['secs_feb17'][idx_1]\n",
    "ul_2['cVp_secs'][~idx_1] = ul_2['secs_feb17'][~idx_1]/ul_2['secs_jan17'][~idx_1]\n",
    "\n",
    "ul_2['pVpp_secs'] = 0\n",
    "idx_1 = (ul_2['secs_dec16'] == 0) \n",
    "ul_2['pVpp_secs'][idx_1] = ul_2['secs_jan17'][idx_1]\n",
    "ul_2['pVpp_secs'][~idx_1] = ul_2['secs_jan17'][~idx_1]/ul_2['secs_dec16'][~idx_1]\n",
    "\n",
    "# is_churn in April\n",
    "ul_3 = user_logs_3.copy()\n",
    "\n",
    "ul_3['cVp_entries'] = 0\n",
    "idx_1 = (ul_3['entries_feb17'] == 0) \n",
    "ul_3['cVp_entries'][idx_1] = ul_3['entries_mar17'][idx_1]\n",
    "ul_3['cVp_entries'][~idx_1] = ul_3['entries_mar17'][~idx_1]/ul_3['entries_feb17'][~idx_1]\n",
    "\n",
    "ul_3['pVpp_entries'] = 0\n",
    "idx_1 = (ul_3['entries_jan17'] == 0) \n",
    "ul_3['pVpp_entries'][idx_1] = ul_3['entries_feb17'][idx_1]\n",
    "ul_3['pVpp_entries'][~idx_1] = ul_3['entries_feb17'][~idx_1]/ul_3['entries_jan17'][~idx_1]\n",
    "\n",
    "ul_3['cVp_secs'] = 0\n",
    "idx_1 = (ul_3['secs_feb17'] == 0) \n",
    "ul_3['cVp_secs'][idx_1] = ul_3['secs_mar17'][idx_1]\n",
    "ul_3['cVp_secs'][~idx_1] = ul_3['secs_mar17'][~idx_1]/ul_3['secs_feb17'][~idx_1]\n",
    "\n",
    "ul_3['pVpp_secs'] = 0\n",
    "idx_1 = (ul_3['secs_jan17'] == 0) \n",
    "ul_3['pVpp_secs'][idx_1] = ul_3['secs_feb17'][idx_1]\n",
    "ul_3['pVpp_secs'][~idx_1] = ul_3['secs_feb17'][~idx_1]/ul_3['secs_jan17'][~idx_1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno             0\n",
      "entries          0\n",
      "entries_dec16    0\n",
      "entries_jan17    0\n",
      "entries_nov16    0\n",
      "num_100          0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_unq          0\n",
      "secs_dec16       0\n",
      "secs_jan17       0\n",
      "secs_nov16       0\n",
      "total_secs       0\n",
      "behaviour        0\n",
      "is_churn         0\n",
      "cVp_entries      0\n",
      "pVpp_entries     0\n",
      "cVp_secs         0\n",
      "pVpp_secs        0\n",
      "dtype: int64\n",
      "msno             0\n",
      "entries          0\n",
      "entries_dec16    0\n",
      "entries_jan17    0\n",
      "entries_nov16    0\n",
      "num_100          0\n",
      "num_25           0\n",
      "num_50           0\n",
      "num_75           0\n",
      "num_985          0\n",
      "num_unq          0\n",
      "secs_dec16       0\n",
      "secs_jan17       0\n",
      "secs_nov16       0\n",
      "total_secs       0\n",
      "behaviour        0\n",
      "is_churn         0\n",
      "cVp_entries      0\n",
      "pVpp_entries     0\n",
      "cVp_secs         0\n",
      "pVpp_secs        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ul_1.isnull().sum())\n",
    "print(ul_1[ul_1 == np.inf].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 21)\n",
      "(970960, 23)\n",
      "(907471, 25)\n"
     ]
    }
   ],
   "source": [
    "print(ul_1.shape)\n",
    "print(ul_2.shape)\n",
    "print(ul_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Synchronizing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we need to make sure that the date related features in our datasets are not in absolut terms (e.g. feb17) but instead relative to the date where is_churn is derived from (e.g. m1 = 1 month before). Indeed, entries from Feb-17 should have the same meaning for is_churn in Mar-17 than entries from Mar-17 for is_churn in Apr-17.\n",
    "Thus we will rename the features so that we get the following structure:\n",
    "- entries_m1 = entries in the month before is_churn is tested\n",
    "- entries_m2 = entries in the month prior prior to where is_churn is tested\n",
    "- secs_m1 and secs_m2: same logic as above\n",
    "\n",
    "Also, to make sure every file has the same amount of features, we had to delete the features of entries and total_secs that where not within three months of the is_churn date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_1_final = ul_1.rename(index=str, columns={\"entries_jan17\": \"entries_m1\", \n",
    "                                                     \"entries_dec16\": \"entries_m2\", \n",
    "                                                     \"entries_nov16\": \"entries_m3\", \n",
    "                                                    \"secs_jan17\": \"secs_m1\",\n",
    "                                                    \"secs_dec16\": \"secs_m2\",\n",
    "                                                    \"secs_nov16\": \"secs_m3\"})\n",
    "\n",
    "ul_2_final = ul_2.drop(columns=['entries_nov16','secs_nov16'],axis=1)\n",
    "ul_2_final = ul_2_final.rename(index=str, columns={\"entries_feb17\": \"entries_m1\", \n",
    "                                                     \"entries_jan17\": \"entries_m2\",\n",
    "                                                   \"entries_dec16\": \"entries_m3\",\n",
    "                                                    \"secs_feb17\": \"secs_m1\",\n",
    "                                                    \"secs_jan17\": \"secs_m2\",\n",
    "                                                  \"secs_dec16\": \"secs_m3\"})\n",
    "\n",
    "ul_3_final = ul_3.drop(columns=['entries_dec16','secs_dec16','entries_nov16','secs_nov16'],axis=1)\n",
    "ul_3_final = ul_3_final.rename(index=str, columns={\"entries_mar17\": \"entries_m1\", \n",
    "                                                     \"entries_feb17\": \"entries_m2\",\n",
    "                                                   \"entries_jan17\": \"entries_m3\",\n",
    "                                                    \"secs_mar17\": \"secs_m1\",\n",
    "                                                    \"secs_feb17\": \"secs_m2\",\n",
    "                                                  \"secs_jan17\": \"secs_m3\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the procedures above, our three datasets now have exactly the same features and can be used to train/test on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 21)\n",
      "(970960, 21)\n",
      "(907471, 21)\n"
     ]
    }
   ],
   "source": [
    "print(ul_1_final.shape)\n",
    "print(ul_2_final.shape)\n",
    "print(ul_3_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           msno  num_25  num_50  num_75  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=   191.0    90.0    75.0   \n",
      "1  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=   207.0   163.0   100.0   \n",
      "2  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=   105.0    24.0    39.0   \n",
      "3  ++0/NopttBsaAn6qHZA2AWWrDg7Me7UOMs1vsyo4tSI=    21.0     8.0    17.0   \n",
      "4  ++0BJXY8tpirgIhJR14LDM1pnaRosjD1mdO1mIKxlJA=    27.0    15.0    10.0   \n",
      "\n",
      "   num_985  num_100  num_unq  total_secs  entries  entries_m1    ...      \\\n",
      "0    144.0    589.0    885.0  192527.892     31.0        31.0    ...       \n",
      "1     64.0    436.0    828.0  149896.558     21.0        21.0    ...       \n",
      "2     35.0    479.0    230.0  116433.247     29.0        29.0    ...       \n",
      "3      7.0    104.0    115.0   28450.268      8.0         8.0    ...       \n",
      "4      4.0    115.0    163.0   31788.296      9.0         9.0    ...       \n",
      "\n",
      "   behaviour     secs_m2     secs_m3  entries_m2  entries_m3  is_churn  \\\n",
      "0   0.848896  156533.957  157868.677        27.0        30.0         0   \n",
      "1   1.224771  123768.839  215757.136        20.0        27.0         0   \n",
      "2   0.423800   78863.188  106043.142        25.0        27.0         0   \n",
      "3   0.509615    5712.573   22640.344         4.0         7.0         0   \n",
      "4   0.486957   23387.903   15738.694         5.0         9.0         0   \n",
      "\n",
      "   cVp_entries  pVpp_entries  cVp_secs  pVpp_secs  \n",
      "0     1.148148      0.900000  1.229943   0.991545  \n",
      "1     1.050000      0.740741  1.211101   0.573649  \n",
      "2     1.160000      0.925926  1.476395   0.743690  \n",
      "3     2.000000      0.571429  4.980290   0.252318  \n",
      "4     1.800000      0.555556  1.359177   1.486013  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "                                           msno  entries  entries_m3  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=    103.0        31.0   \n",
      "1  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=    609.0        31.0   \n",
      "2  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=    603.0        24.0   \n",
      "3  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=    292.0        26.0   \n",
      "4  ++/UDNo9DLrxT8QVGiDi1OnWfczAdEwThaVyD0fXO50=    445.0        18.0   \n",
      "\n",
      "   entries_m1  entries_m2  num_100  num_25  num_50  num_75  num_985  \\\n",
      "0        27.0        30.0   2231.0   572.0   289.0   284.0    504.0   \n",
      "1        28.0        31.0  15572.0  1396.0   494.0   420.0    561.0   \n",
      "2        20.0        27.0  23807.0  6688.0  2213.0  1476.0   1402.0   \n",
      "3        25.0        27.0   5092.0  1438.0   449.0   350.0    471.0   \n",
      "4        18.0        27.0   4803.0  1083.0   540.0   299.0    415.0   \n",
      "\n",
      "     ...         secs_m3     secs_m1     secs_m2   total_secs  behaviour  \\\n",
      "0    ...      256812.004  156533.957  157868.677   719882.711   0.739130   \n",
      "1    ...      154978.397  189007.847  198105.280  4061562.584   0.184369   \n",
      "2    ...      169086.593  123768.839  215757.136  6683044.760   0.494770   \n",
      "3    ...      132765.529   78863.188  106043.142  1485801.348   0.531815   \n",
      "4    ...       54255.659   49219.147   51758.313  1398801.748   0.486571   \n",
      "\n",
      "   is_churn  cVp_entries  pVpp_entries  cVp_secs  pVpp_secs  \n",
      "0         0     0.900000      0.967742  0.991545   0.614725  \n",
      "1         0     0.903226      1.000000  0.954078   1.278277  \n",
      "2         0     0.740741      1.125000  0.573649   1.276016  \n",
      "3         0     0.925926      1.038462  0.743690   0.798725  \n",
      "4         0     0.666667      1.500000  0.950942   0.953971  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "                                           msno  entries  entries_m2  \\\n",
      "0  +++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=    103.0        31.0   \n",
      "1  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=    609.0        31.0   \n",
      "2  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=    603.0        24.0   \n",
      "3  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=    292.0        26.0   \n",
      "4  ++/UDNo9DLrxT8QVGiDi1OnWfczAdEwThaVyD0fXO50=    445.0        18.0   \n",
      "\n",
      "   entries_m1  entries_m3  num_100  num_25  num_50  num_75  num_985  \\\n",
      "0        30.0        15.0   2231.0   572.0   289.0   284.0    504.0   \n",
      "1        31.0        29.0  15572.0  1396.0   494.0   420.0    561.0   \n",
      "2        27.0        17.0  23807.0  6688.0  2213.0  1476.0   1402.0   \n",
      "3        27.0        26.0   5092.0  1438.0   449.0   350.0    471.0   \n",
      "4        27.0        19.0   4803.0  1083.0   540.0   299.0    415.0   \n",
      "\n",
      "     ...         secs_m2     secs_m1     secs_m3   total_secs  behaviour  \\\n",
      "0    ...      256812.004  157868.677  148668.073   719882.711   0.739130   \n",
      "1    ...      154978.397  198105.280  133337.861  4061562.584   0.184369   \n",
      "2    ...      169086.593  215757.136  289423.141  6683044.760   0.494770   \n",
      "3    ...      132765.529  106043.142  129382.878  1485801.348   0.531815   \n",
      "4    ...       54255.659   51758.313   29430.154  1398801.748   0.486571   \n",
      "\n",
      "   is_churn  cVp_entries  pVpp_entries  cVp_secs  pVpp_secs  \n",
      "0         0     0.967742      2.066667  0.614725   1.727419  \n",
      "1         0     1.000000      1.068966  1.278277   1.162299  \n",
      "2         0     1.125000      1.411765  1.276016   0.584219  \n",
      "3         0     1.038462      1.000000  0.798725   1.026145  \n",
      "4         0     1.500000      0.947368  0.953971   1.843540  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ul_3_final.head())\n",
    "print(ul_2_final.head())\n",
    "print(ul_1_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we make sure that the order of the features is the same for every file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_3_final['is_churn'] = 0\n",
    "ul_2_final = ul_2_final[ul_1_final.columns]\n",
    "ul_3_final = ul_3_final[ul_1_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "hmm = ul_2_final.columns == ul_3_final.columns\n",
    "print(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downloading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (1/3)!\n",
      "Done (2/3)!\n",
      "Done (3/3)!\n"
     ]
    }
   ],
   "source": [
    "# download file\n",
    "ul_1_final.to_csv('data/user_logs_feb.csv', index=False)\n",
    "print(\"Done (1/3)!\")\n",
    "ul_2_final.to_csv('data/user_logs_mar.csv', index=False)\n",
    "print(\"Done (2/3)!\")\n",
    "ul_3_final.to_csv('data/user_logs_apr.csv', index=False)\n",
    "print(\"Done (3/3)!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
